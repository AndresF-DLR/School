{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection APIs & Machine Learning - A Brief Look\n",
    "*By Andrés De los Ríos -  26/02/2019*\n",
    "\n",
    "MSL2030: Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Intro - Data & Museums: Why Machine Learning?](#1)\n",
    "2. [Tools](#2)\n",
    "3. [APIs - Collecting Data](#3)\n",
    "4. [Exploring the Data](#4)\n",
    "5. [Preprocessing](#5)\n",
    "6. [Feature Extraction](#6)\n",
    "7. [Tf-idf](#7)\n",
    "8. [Naive Bayes + Tf-idf](#8)\n",
    "9. [Random Forest + Tf-idf](#9)\n",
    "10. [Comparing Results & Improving the Model](#10)\n",
    "11. [Conclusion](#11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a name=\"1\"></a>\n",
    "\n",
    "### Data & Museums: Why Machine Learning?\n",
    "\n",
    "Machine learning has proven to be a paradoxical tool in the modern workforce: despite its complex mathematical foundations, [companies and employees from all sectors are quickly integrating machine learning into new business strategies or products.][1] The world of art and museums has not been an exception. \n",
    "\n",
    "In the private sector, artificial intelligence (AI) has defied our definition of creativity and authorship by [producing artworks worth of close half a million dollars.][2] \n",
    "\n",
    "Museums have also implemented AI to create new connections amongst their objects. For example, just last year the Barnes Foundation incorporated computer vision to [their public collections website][3] so that users can browse artworks according to visual aspects such as linear composition, color, and spatial arrangement. (In a way, the new set-up [further manifests the museum's challenge][4] towards any predetermined art-historical groupings that may be imposed to audiences during their visit.)\n",
    "\n",
    "Similarly, the Baseball Hall of Fame [used Python and machine learning scripts to automatically tag their collection's online content][5], thereby bringing to the fore pieces and stories that had been previously kept in the dark. \n",
    "\n",
    "To get staff started on the endless possibilities of machine learning, this tutorial offer an introductory glimpse into the processes of collections data acquisition, processing and analysis through two prominent classification models. \n",
    "\n",
    "### Supervised Machine Learning: Clasification \n",
    "\n",
    "Before coding, we need to set some vocabulary straight:\n",
    "\n",
    "* **Supervised Machine Learning** are any algorithms that use patterns previously-labelled data and its characteristics or features to tag unlabeled entries.  \n",
    "\n",
    "* **Classification** is the technique of determining a data entry's class/label/tag based on one or more independent variables. \n",
    "\n",
    "For the present excercise, we will use two different classification models: **Naive Bayes** and **Random Forest** - explained below. In our case, the object entries will be our records; the content of their label texts will be processed as independent variables; and their place of origin will be the predicted tags. \n",
    "\n",
    "Properly used, these tools can help collections staff automatically classify their vast holdings. Furthermore, the data processes and machine learning models could also be used by IT, Education and Interpretation Departments to challenge traditional perspectives of their institution's collection. \n",
    "\n",
    "[1]: https://towardsdatascience.com/essential-data-skills-supply-and-demand-on-the-job-market-4f7dffa23b70\n",
    "[2]: https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx\n",
    "[3]: https://collection.barnesfoundation.org\n",
    "[4]: https://medium.com/barnes-foundation/rethinking-the-museum-collection-online-e3b864d8bb39\n",
    "[5]: https://blog.cogapp.com/using-artificial-intelligence-to-enhance-user-experience-201370248b94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tools <a name=\"2\"></a>\n",
    "\n",
    "### Tutorial Requirements\n",
    "* Basic knowledge of Python - see [CognitiveClass.ai's \"Python 101\" course][1] or [DataCamp.com's Data Analyst tutorials][2] for more introductory material on using Python to extract and manipulate data. \n",
    "* Python Integrated Development Environment (IDE) - for example Jupyter Notebooks, Visual Studio Code or Sypder. \n",
    "* Working collections API - in this excercise, we will use the [Rijksmuseum's API][3]; see the linked documentation for more informatin on the API's request and data structure. \n",
    "* A collections dataset - for best results, ensure each record has at least one associated label/descriptive text and categorical feature (for example a location, category, or type tag). \n",
    "[1]:  https://cognitiveclass.ai/courses/python-for-data-science/\n",
    "[2]: https://www.datacamp.com/tracks/data-analyst-with-python\n",
    "[3]: http://rijksmuseum.github.io\n",
    "\n",
    "### Python Modules and Libraries\n",
    "To run our code, we will need to download and import the following modules into our Python environment: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json #to easily navigate json data objects extracted from the Rijksmuseum's API. \n",
    "import matplotlib.pyplot as plt #to visually explore our dataset.\n",
    "import pandas as pd #to manipulate our data as a single dataframe.\n",
    "import re #to clean the strings in our dataset through regular expressions.\n",
    "import requests #to make the HTTP requests needed to extract the data we want from the Rijksmuseum's API.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #to convert our our label text into a matrix of features\n",
    "from sklearn.naive_bayes import MultinomialNB #multinomnial Naive Bayes classifier \n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest classifier\n",
    "from sklearn.model_selection import train_test_split #to quickly split our data into random train and test subsets. \n",
    "from sklearn.metrics import classification_report, accuracy_score #to easily represent and compare the accuracy of our models. \n",
    "from sklearn import preprocessing #to change raw features into suitable representations for our models \n",
    "from nltk.corpus import stopwords #a compilation of English stopwords to filter out irrelevant features for our classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. APIs - Collecting Data <a name=\"3\"></a>\n",
    "\n",
    "**Application Programming Interface (API)**: a set of protocols and commands to \"communicate\" with a particular program. In the case of online collections, the API takes the form of a url address with several optional parameters that dictate how your query will search the collection and the output of said search. \n",
    "\n",
    "Some examples of online collection APIs include:\n",
    "- [Metropolitan Museum of Art][1]\n",
    "- [Cooper-Hewitt Smithsonian Design Museum][2]\n",
    "- [Harvard Art Museums][3]\n",
    "- [Victoria & Albert Museum][4]\n",
    "\n",
    "\n",
    "### Constants\n",
    "Before making any requests, we need to initialize the constants we will use throughout the tutorial. These are any labels, columns, keys or object information that will not change even after processing the code's commands. \n",
    "\n",
    "These constants often depend on each museum's collection metadata or each API's formatting, so look carefully into your institution's documentation to use the proper labels and keys. \n",
    "\n",
    "In the example below, we have selected a number of object features drawn from the Rijksmuseum API documentation; these will later serve as our dataframe's columns. Likewise, we have obtained an API key from the museum that validates our request for collections information. \n",
    "\n",
    "[1]: https://metmuseum.github.io\n",
    "[2]: https://collection.cooperhewitt.org/api/\n",
    "[3]: https://github.com/harvardartmuseums/api-docs\n",
    "[4]: http://collections.vam.ac.uk/information/information_apigettingstarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object feature categories to extract from each request; these will later serve as our data frame's columns.\n",
    "r_columns_db = ['objectNumber', \"links\", \"title\", \"principalOrFirstMaker\", 'longTitle','productionPlaces']\n",
    "\n",
    "#Unique API key to request data from the Rijksmuseum API - each user needs to solicit their own from the museum. \n",
    "r_API = \"auGMKu87\"\n",
    "\n",
    "#URL through which we will query data from the Rijksmuseum's API. \n",
    "r_path = \"https://www.rijksmuseum.nl/api/en/collection/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions \n",
    "\n",
    "Three functions will help us extract data from the Rijksmuseum API: \n",
    "1. **r_Search_Works_Display** searches the collection and pulls the unique identifiers (the \"objectNumber\" feature) and URLs of any artworks on display that relate to our query. \n",
    "2. **r_to_DF** creates a data frame from our object listing where each artefact represents a row and the features of the constant r_columns_db represent the dataframe's columns. \n",
    "3. **get_Object_Info** queries each object's complete set of records through its unique identifier to populate the dataframe with the column features specified above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r_Search_Works_Display\n",
    "\n",
    "The function first produces a lower-case copy of the query the API will use to search the collections **(1)**. This a common practice when handling text data as it helps standarize how we query information.  \n",
    "\n",
    "We must then specify the parameters for our query **(2)**:\n",
    "* \"key\": our unique key that proves we received permission to query the Rijksmiseum's API.\n",
    "* \"format\": format of the query's output. \n",
    "* \"q\": string of text we want the API to search for in the collection's object records; if left blank, the API will just pull information from all objects that are currently on display.\n",
    "* \"ps\": number of results per page\n",
    "* \"ondisplay\": forces the request to only produce objects that are (or not) on display. \n",
    "* \"st\": describes the category of records where the search will take place. \n",
    "\n",
    "*NOTE: these represent only a handful of the Rijksmuseum's API request parameters, for more information on the other options, please see the museum's documentation.*   \n",
    "\n",
    "The count of first request will tell us the number of objects that match our query; we will then divide this amount by 100 to obtain the number of pages we will need to parse through to collect data on all objects **(3)**\n",
    "\n",
    "After initializing the dictionary where we will store the object's basic information **(4)**, we will issue a new set of requests - this time, we will specify the page of object listings we want to parse. **(5)** Each page's content is extracted as a json package containing 100 object records. **(6)** In this step, we are looking specifically for the objects' unique identifiers, which will serve as their keys in the dictionary we initialized above; the object's basic information will serve as its key's value in the dictionary **(7)**\n",
    "\n",
    "Once the code has parsed through all the pages and all the objects, the function returns a full dictionary of data. **(8)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_Search_Works_Display(query):\n",
    "\n",
    "    #1\n",
    "    query = query.strip().lower()\n",
    "    \n",
    "    #2\n",
    "    query_params = {'key': r_API, 'format': 'json', 'q': query, 'ps': '100', \"ondisplay\": \"True\", \"st\": \"Objects\"}\n",
    "    \n",
    "    #3\n",
    "    r_count = requests.get(r_path, params=query_params)\n",
    "    \n",
    "    r_pages = (r_count.json()[\"count\"]//100) + 1\n",
    "    \n",
    "    #4\n",
    "    r_works = {}\n",
    "    \n",
    "    #5\n",
    "    while r_pages != 0:\n",
    "        \n",
    "        query_params[\"p\"] = str(r_pages)\n",
    "        \n",
    "        r_page_request = requests.get(r_path, params=query_params)\n",
    "        \n",
    "        #6\n",
    "        objects = r_page_request.json()\n",
    "        \n",
    "        for i in range(len(objects['artObjects'])):\n",
    "            \n",
    "            #7\n",
    "            objectID = objects['artObjects'][i][\"objectNumber\"]\n",
    "            \n",
    "            r_works[objectID] = objects['artObjects'][i]\n",
    "            \n",
    "        r_pages -= 1\n",
    "        \n",
    "    #8    \n",
    "    return r_works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r_To_DF\n",
    "\n",
    "We must now use a function to change our data dictionary into a dataframe for easy manipulation. **(1)** Including the constant list of features specified above will as a function parameter integrate them into the dataframe as columns. **(2)** (In case we ever need to access an object's web page, we can extract the website's url from the \"links\" feature.) **(3)**.  \n",
    "\n",
    "In the end, the function returns a dataframe with objects as rows, object numbers as their unique identifiers and features as columns. **(4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_To_DF(ObjectData, columns = \"\"):\n",
    "    \n",
    "    #1\n",
    "    df = pd.DataFrame.from_dict(ObjectData, orient = \"index\")\n",
    "    \n",
    "    #2\n",
    "    if columns:\n",
    "        df = df[columns]\n",
    "    \n",
    "    #3\n",
    "    df[\"url\"] = df[\"links\"].apply(lambda x: x[\"web\"])\n",
    "    \n",
    "    df = df.drop(columns=['links'])\n",
    "    \n",
    "    #4\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_Object_Info\n",
    "\n",
    "To obtain an object's complete record - which includes additional information such as label, media, type, etc. - we need to create a request that includes its unique identifier. **(1)**\n",
    "\n",
    "We must now travel through each request's resulting json package to pull specific object information. In this case, we have located and extracted each artefact's label text, type, collection category, material and iconographic description. **(2)** These variables are then passed as data to populate each object's row in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Object_Info(row):\n",
    "\n",
    "    #1\n",
    "    i = row[\"objectNumber\"]\n",
    "    \n",
    "    object_entry = requests.get(r_path + i + \"?key=\" + r_API+ \"&format=json\")\n",
    "    \n",
    "    #2\n",
    "    object_label = object_entry.json()[\"artObject\"][\"label\"][\"description\"]\n",
    "    \n",
    "    object_types = object_entry.json()[\"artObject\"]['objectTypes']\n",
    "    \n",
    "    object_collections = object_entry.json()[\"artObject\"]['objectCollection']\n",
    "    \n",
    "    object_materials = object_entry.json()[\"artObject\"]['materials']\n",
    "    \n",
    "    object_icon_class = object_entry.json()[\"artObject\"][\"classification\"]['iconClassDescription']\n",
    "    \n",
    "    return object_label, object_types, object_collections, object_materials, object_icon_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Now we will process the functions in the appropriate sequence. \n",
    "\n",
    "Notice we need to specify how we will label the outputs of function r_get_Object_Info according to the information we pulled from each object's complete record. \n",
    "\n",
    "We also had to make use of pandas' **apply** function alongside **zip()** (and it's * operator): with \"axis=1\", the former will apply the specified function to each row in the dataframe; the latter will pair the new columns with their associated function outputs every time the function runs. **(1)**\n",
    "\n",
    "Finally, since we will rely on label text for our machine learning analysis, we need to quickly filter out any rows without a label. **(2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r_Search_Works_Display(\"\")\n",
    "\n",
    "df = r_To_DF(data, columns=r_columns_db)\n",
    "\n",
    "#1\n",
    "df[\"Label\"], df[\"Type\"], df[\"Collection\"], df[\"Materials\"], df[\"Icons\"] = zip(*df.apply(get_Object_Info, axis=1))\n",
    "\n",
    "#df = pd.read_csv(\"sample_data_Displayed.csv\", encoding = \"ISO-8859-1\") - for testing purposes only\n",
    "\n",
    "#2\n",
    "df = df[df[\"Label\"].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploring the Data <a name=\"4\"></a>\n",
    "Having collected our data, we can use various basic functions to explore the dataset's main features: \n",
    "- Using the **head** function we can look at the first five rows of our dataset (or more, if specified); this is a good way of ensuring the data obtained from our homemade functions looks as expected. **(1)** \n",
    "- The **describe** function will display how many rows have information for each of our columns as well as the number of unique values for each feature and the most common value's frequency. **(2)**\n",
    "- Look into specific column values with **iloc** to ensure the data is in a type you need. **(3)**\n",
    "- We can also count how many individual words exists in total within the Label field of our dataset - a good metric to consider when conducting Natural Language Processing excercises. **(4)**\n",
    "- If we wanted to consider other features as variables in our analysis, we filter our dataset's rows by combining conditional statements **(5)**\n",
    "- With **plotly** we can visualize certain features of our dataset such as the five most common collection categories **(6)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectNumber</th>\n",
       "      <th>title</th>\n",
       "      <th>principalOrFirstMaker</th>\n",
       "      <th>longTitle</th>\n",
       "      <th>productionPlaces</th>\n",
       "      <th>url</th>\n",
       "      <th>Label</th>\n",
       "      <th>Type</th>\n",
       "      <th>Collection</th>\n",
       "      <th>Materials</th>\n",
       "      <th>Icons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AK-BR-GRO-1</th>\n",
       "      <td>AK-BR-GRO-1</td>\n",
       "      <td>Seated Buddha</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>Seated Buddha, anonymous, c. 1700 - c. 1800</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.rijksmuseum.nl/en/collection/AK-BR...</td>\n",
       "      <td>This Buddha makes the same gesture as the adja...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[sculptures]</td>\n",
       "      <td>[bronze (metal)]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK-BR-JAN-1</th>\n",
       "      <td>AK-BR-JAN-1</td>\n",
       "      <td>Shotoku Taishi</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>Shotoku Taishi, anonymous, c. 1200 - c. 1350</td>\n",
       "      <td>[Japan]</td>\n",
       "      <td>https://www.rijksmuseum.nl/en/collection/AK-BR...</td>\n",
       "      <td>Buddhism took hold in Japan in the middle of t...</td>\n",
       "      <td>[figure]</td>\n",
       "      <td>[sculptures]</td>\n",
       "      <td>[wood (plant material)]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK-C-2012-28</th>\n",
       "      <td>AK-C-2012-28</td>\n",
       "      <td>Document case</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>Document case, anonymous, 1684 - 1692</td>\n",
       "      <td>[Japan]</td>\n",
       "      <td>https://www.rijksmuseum.nl/en/collection/AK-C-...</td>\n",
       "      <td>This document case is a fine example of the ki...</td>\n",
       "      <td>[document]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[wood (plant material), lacquer (coating)]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK-MAK-1168</th>\n",
       "      <td>AK-MAK-1168</td>\n",
       "      <td>Two Images of Vajrasattva and a Vajra</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>Two Images of Vajrasattva and a Vajra, anonymo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.rijksmuseum.nl/en/collection/AK-MA...</td>\n",
       "      <td>The thunderbolt, or vajra, is a powerful symbo...</td>\n",
       "      <td>[figure]</td>\n",
       "      <td>[sculptures]</td>\n",
       "      <td>[bronze (metal)]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK-MAK-1186</th>\n",
       "      <td>AK-MAK-1186</td>\n",
       "      <td>Fragment of a doorpost</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>Fragment of a doorpost, anonymous, 200 - 300</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.rijksmuseum.nl/en/collection/AK-MA...</td>\n",
       "      <td>This stone is carved with two scenes from the ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[sculptures]</td>\n",
       "      <td>[sandstone]</td>\n",
       "      <td>[Buddha, founder of Buddhism]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              objectNumber                                  title  \\\n",
       "AK-BR-GRO-1    AK-BR-GRO-1                          Seated Buddha   \n",
       "AK-BR-JAN-1    AK-BR-JAN-1                         Shotoku Taishi   \n",
       "AK-C-2012-28  AK-C-2012-28                          Document case   \n",
       "AK-MAK-1168    AK-MAK-1168  Two Images of Vajrasattva and a Vajra   \n",
       "AK-MAK-1186    AK-MAK-1186                 Fragment of a doorpost   \n",
       "\n",
       "             principalOrFirstMaker  \\\n",
       "AK-BR-GRO-1              anonymous   \n",
       "AK-BR-JAN-1              anonymous   \n",
       "AK-C-2012-28             anonymous   \n",
       "AK-MAK-1168              anonymous   \n",
       "AK-MAK-1186              anonymous   \n",
       "\n",
       "                                                      longTitle  \\\n",
       "AK-BR-GRO-1         Seated Buddha, anonymous, c. 1700 - c. 1800   \n",
       "AK-BR-JAN-1        Shotoku Taishi, anonymous, c. 1200 - c. 1350   \n",
       "AK-C-2012-28              Document case, anonymous, 1684 - 1692   \n",
       "AK-MAK-1168   Two Images of Vajrasattva and a Vajra, anonymo...   \n",
       "AK-MAK-1186        Fragment of a doorpost, anonymous, 200 - 300   \n",
       "\n",
       "             productionPlaces  \\\n",
       "AK-BR-GRO-1                []   \n",
       "AK-BR-JAN-1           [Japan]   \n",
       "AK-C-2012-28          [Japan]   \n",
       "AK-MAK-1168                []   \n",
       "AK-MAK-1186                []   \n",
       "\n",
       "                                                            url  \\\n",
       "AK-BR-GRO-1   https://www.rijksmuseum.nl/en/collection/AK-BR...   \n",
       "AK-BR-JAN-1   https://www.rijksmuseum.nl/en/collection/AK-BR...   \n",
       "AK-C-2012-28  https://www.rijksmuseum.nl/en/collection/AK-C-...   \n",
       "AK-MAK-1168   https://www.rijksmuseum.nl/en/collection/AK-MA...   \n",
       "AK-MAK-1186   https://www.rijksmuseum.nl/en/collection/AK-MA...   \n",
       "\n",
       "                                                          Label        Type  \\\n",
       "AK-BR-GRO-1   This Buddha makes the same gesture as the adja...          []   \n",
       "AK-BR-JAN-1   Buddhism took hold in Japan in the middle of t...    [figure]   \n",
       "AK-C-2012-28  This document case is a fine example of the ki...  [document]   \n",
       "AK-MAK-1168   The thunderbolt, or vajra, is a powerful symbo...    [figure]   \n",
       "AK-MAK-1186   This stone is carved with two scenes from the ...          []   \n",
       "\n",
       "                Collection                                   Materials  \\\n",
       "AK-BR-GRO-1   [sculptures]                            [bronze (metal)]   \n",
       "AK-BR-JAN-1   [sculptures]                     [wood (plant material)]   \n",
       "AK-C-2012-28            []  [wood (plant material), lacquer (coating)]   \n",
       "AK-MAK-1168   [sculptures]                            [bronze (metal)]   \n",
       "AK-MAK-1186   [sculptures]                                 [sandstone]   \n",
       "\n",
       "                                      Icons  \n",
       "AK-BR-GRO-1                              []  \n",
       "AK-BR-JAN-1                              []  \n",
       "AK-C-2012-28                             []  \n",
       "AK-MAK-1168                              []  \n",
       "AK-MAK-1186   [Buddha, founder of Buddhism]  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                  2973\n",
       "unique                                                 2076\n",
       "top       Manufactuur Oud-Loosdrecht\\nLoosdrecht, c. 177...\n",
       "freq                                                     54\n",
       "Name: Label, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2\n",
    "df['Label'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Italy'] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "print(df['productionPlaces'].iloc[300], type(df['productionPlaces'].iloc[300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152192\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "print(df['Label'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "df_2 = df[(df[\"Collection\"].map(lambda x: len(x)) > 0) &\n",
    "   (df[\"Icons\"].map(lambda x: len(x)) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andres/anaconda/envs/Py3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#6\n",
    "df_2[\"Main_Collection\"] = df_2[\"Collection\"].map(lambda x: x[0])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (12, 5))\n",
    "df_2.groupby('Main_Collection')[\"objectNumber\"].count().sort_values()[-5:].plot.bar(ylim=0)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Preprocessing <a name=\"5\"></a>\n",
    "\n",
    "### Dataframe Preprocessing\n",
    "\n",
    "We now have to clean our data - especifically the fields we will use for the analysis - \"productionPlaces\" and \"Label\".\n",
    "\n",
    "We will first create a copy of our dataframe: this is so we can make changes to our place information without disrupting our original dataset. The copy will also filter out any rows that do not have either any label or origin information **(1)**\n",
    "\n",
    "Now we can select the most relevant location of each row by creating a new column for the first element of the lists in each row's \"productionPlaces\" feature. **(2)** For our classifier model to work, we will also need to remove any rows with locations that have less than one occurrence. **(3)**\n",
    "\n",
    "Finally, we will limit the number of columns in the dataset to the features essential to our analysis **(4)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "df_Places = df[(df[\"productionPlaces\"].map(lambda x: len(x)) > 0) &\n",
    "  (df[\"Label\"].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andres/anaconda/envs/Py3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "df_Places[\"Place\"] = df_Places[\"productionPlaces\"].map(lambda x: x[0])\n",
    "\n",
    "#3\n",
    "df_Places = df_Places[df_Places.groupby(\"Place\").objectNumber.transform(len) > 1]\n",
    "\n",
    "#4\n",
    "df_Places = df_Places[[\"objectNumber\", \"longTitle\", \"Label\", \"Place\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly look at our new dataset - the one will will use for training and testing our models - to make sure the data is what we expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectNumber</th>\n",
       "      <th>longTitle</th>\n",
       "      <th>Label</th>\n",
       "      <th>Place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AK-BR-JAN-1</th>\n",
       "      <td>AK-BR-JAN-1</td>\n",
       "      <td>Shotoku Taishi, anonymous, c. 1200 - c. 1350</td>\n",
       "      <td>Buddhism took hold in Japan in the middle of t...</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK-C-2012-28</th>\n",
       "      <td>AK-C-2012-28</td>\n",
       "      <td>Document case, anonymous, 1684 - 1692</td>\n",
       "      <td>This document case is a fine example of the ki...</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK-MAK-119</th>\n",
       "      <td>AK-MAK-119</td>\n",
       "      <td>Seishi, anonymous, c. 1400 - c. 1600</td>\n",
       "      <td>The deity Seishi stands for wisdom and in Japa...</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK-MAK-12</th>\n",
       "      <td>AK-MAK-12</td>\n",
       "      <td>Ornament for a shield, anonymous, c. -1100 - c...</td>\n",
       "      <td>Bronze human masks were affixed to shields as ...</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK-MAK-121</th>\n",
       "      <td>AK-MAK-121</td>\n",
       "      <td>The Buddha Amida Nyorai, anonymous, c. 1500 - ...</td>\n",
       "      <td>This Buddha is holding his hands on his lap in...</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              objectNumber                                          longTitle  \\\n",
       "AK-BR-JAN-1    AK-BR-JAN-1       Shotoku Taishi, anonymous, c. 1200 - c. 1350   \n",
       "AK-C-2012-28  AK-C-2012-28              Document case, anonymous, 1684 - 1692   \n",
       "AK-MAK-119      AK-MAK-119               Seishi, anonymous, c. 1400 - c. 1600   \n",
       "AK-MAK-12        AK-MAK-12  Ornament for a shield, anonymous, c. -1100 - c...   \n",
       "AK-MAK-121      AK-MAK-121  The Buddha Amida Nyorai, anonymous, c. 1500 - ...   \n",
       "\n",
       "                                                          Label  Place  \n",
       "AK-BR-JAN-1   Buddhism took hold in Japan in the middle of t...  Japan  \n",
       "AK-C-2012-28  This document case is a fine example of the ki...  Japan  \n",
       "AK-MAK-119    The deity Seishi stands for wisdom and in Japa...  Japan  \n",
       "AK-MAK-12     Bronze human masks were affixed to shields as ...  China  \n",
       "AK-MAK-121    This Buddha is holding his hands on his lap in...  Japan  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Places.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "Now we need to clean our string data by removing odd symbols or stopwords that might mess with the classifier's assessment. \n",
    "\n",
    "For this we need to initialize three sets of characters/words using [regular expressions][1]:\n",
    "1. All punctuation that should be replaced in the string with a space **(1)**\n",
    "2. All symbols that are not numbers or alphabetic chracters and should be removed completely - i.e. they should be replaced with nothing. **(2)**\n",
    "3. All English words whose increased frequency in the language could interfere with the classifier's analysis. **(3)**\n",
    "\n",
    "The function **clean_Text** takes strings and removes unnecessary characters/words in them that appear in the sets compiled above. **(4)** We can then use the function to create \"clean\" versions of the two features used in our analysis. **(5)**\n",
    "\n",
    "*Note: for more information in regular expressions, their use and creation, please see the following [O'Reilly article][2] or [video tutorials][3].*\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Regular_expression\n",
    "[2]: https://www.oreilly.com/ideas/an-introduction-to-regular-expressions\n",
    "[3]: https://www.youtube.com/watch?v=7DG3kCDx53c&list=PLRqwX-V7Uu6YEypLuls7iidwHMdCM6o2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "\n",
    "#2\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "#3\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "#4\n",
    "def clean_Text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "df_Places[\"Place\"] = df_Places[\"Place\"].apply(clean_Text)\n",
    "df_Places[\"Label\"] = df_Places[\"Label\"].apply(clean_Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction <a name=\"6\"></a>\n",
    "With our data clean, we can transform it from raw text to features - a core step in any Natural Language Processing analysis.\n",
    "\n",
    "First, **LabelEncoder** will transform categorical string data (our \"Place\" feature) into numerical values our model can understand - i.e. numbers between 0 and n-1, with n representing the total number of unique categories in our feature. **(1)**\n",
    "\n",
    "Then, we use **fit_transform** with our encoded labels to transform our data according to the dataset's centered parameters. **(2)**\n",
    "\n",
    "*Note: while a thorough explanation of the importance of fit_transform is outside the scope of this excercise, there are [online tutorials][1] and [forum posts][2] that go further into the function's relevance.*\n",
    "\n",
    "**train_test_split** will randomly divide our dataset into training and testing subsets **(3)**:\n",
    "* Training Set: labelled/tagged records; the classifier will study their features to learn how to best predict the different categories. \n",
    "* Test Set: unlabelled records; the classifier will try to predict their categories and then compare its predictions with each record's real label. \n",
    "* The output is hence 4 sets of data: the features for the training and testing subsets (X_train, X_test) as well as the labels/categories for the training and testing data (y_train, y_test). \n",
    "\n",
    "In this case, we are telling the function to split our data 90%/10% for the training and testing sets, respectively. \n",
    "\n",
    "Finally, we will extract an array describing the unique classes produced from our label encoding. This will be useful later when we need to point our machine learning models to the categories available for classification. **(4)**\n",
    "\n",
    "\n",
    "[1]: https://www.youtube.com/watch?v=ENvSybznF_o\n",
    "[2]:https://datascience.stackexchange.com/questions/12321/difference-between-fit-and-fit-transform-in-scikit-learn-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "#2\n",
    "y = lbl_enc.fit_transform(df_Places.Place.values)\n",
    "\n",
    "#3\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_Places.Label, y, \n",
    "                                                    stratify = y, \n",
    "                                                    random_state = 42, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "target_names = lbl_enc.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tf-idf <a name=\"7\"></a>\n",
    "\n",
    "### Term Vectorization\n",
    "\n",
    "After preparing our label data, the final step before applying the models is to vectorize our \"Label\" data. \n",
    "\n",
    "Simply put, this will reinterpret each word in our corpus as a column represented by a unique number. Each Label can then be transformed as a row of integers, with each number representing the frequency of its column's word (the term frequency) in the label text. \n",
    "\n",
    "Here is a very simple example: the vector is the first graph, while the other two link each text's and word's key to their English counterpart.\n",
    "\n",
    "\n",
    "Text_ID| 0 | 1 | 2 | 3 | 4 |\n",
    ":--- | --- | --- | --- | --- | --- |\n",
    "Ex1 | 1 | 0 | 1 | 1 | 0 |\n",
    "Ex2 | 1 | 1 | 0 | 1 | 0 |\n",
    "Ex3| 1 | 1 | 0 | 0 | 1 |\n",
    "Ex4 | 1 | 0 | 1 | 0 | 1 |\n",
    "Ex5 | 0 | 2 | 0 | 0 | 1 |\n",
    "\n",
    "Word_ID| Word |\n",
    ":--- | --- |\n",
    "0 | \"I\" |\n",
    "1 | \"red\" |\n",
    "2| \"blue\" |\n",
    "3 | \"like\" |\n",
    "4 | \"love\" |\n",
    "\n",
    "Text_ID| Sentence\n",
    ":--- | --- |\n",
    "Ex1 | \"I like blue\" |\n",
    "Ex2 | \"I like red\" |\n",
    "Ex3| \"I love red\" |\n",
    "Ex4 | \"I love blue\" |\n",
    "Ex5 | \"Red love red\" |\n",
    "\n",
    "\n",
    "### TF-IDF: Basics\n",
    "One of the most common methods of term vectorization is [\"Term Frequency-Inverse Document Frequency\" (TF-IDF)][1]. \n",
    "* Term Frequency (TF): measures how frequently a term occurs in a document. \n",
    "* TF(t): represents how we normalize each term frequency - *(Number of times term t appears in a document) / (Total number of terms in the document)*\n",
    "* Inverse Document Frequency (IDF): measures how important a term is. For this, we weigh down frequent terms while scaling up the rare ones. \n",
    "* IDF(t): computes the weight of each term based on it's popularity accross all our documents/labels -  *log_e(Total number of documents / Number of documents with term t in it)*\n",
    "\n",
    "For our program, we will instantiate a version of scikit learn's **TfidfVectorizer** function **(1)**. While it is always recommended to go through the [documentation][2] to properly understand the function's parameters, here is a quick overview of those we altered for our analysis:\n",
    "* strip_accents='unicode', replace all accented unicode characters with their corresponding ASCII character.\n",
    "* analyzer='word', define our feature type as words.\n",
    "* token_pattern=r'\\w{1,}', tokenize only words of 1 or more characters.\n",
    "* ngram_range=(1, 3), specifies our features are made of [unigram, bigram, and trigram][3].\n",
    "* use_idf=True, enables inverse-document-frequency reweighting.\n",
    "* smooth_idf=True, prevents zero division for unseen words.\n",
    "* sublinear_tf=True, regularization technique that squishes the data to a lower, common magnitude -> 1 + log(tf).\n",
    "* stop_words='english', cleans the text for common, low-impact words to make way for more relevant words that are unique to the corpus.\n",
    "\n",
    "After creating our TF-IDF instance, we can fit the features of our training and testing subsets to have a matrix of all the existing terms in our corpus. **(2)** \n",
    "\n",
    "It is then necessary to transform our arrays of label text: now each row  will contain an number associated to a row in our dataset, a unique integer number (representing an instance of a word in the row) and the word's associated importance score as calculated by TF-IDF. **(3)** \n",
    "\n",
    "For a visual representation of these last steps, see the following [article][4].\n",
    "\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Tf–idf\n",
    "[2]: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "[3]: https://en.wikipedia.org/wiki/N-gram\n",
    "[4]: https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "tfidf = TfidfVectorizer(strip_accents = \"unicode\", analyzer = \"word\",\n",
    "                       token_pattern = r'\\w{1,}', ngram_range = (1, 3),\n",
    "                       use_idf= True, smooth_idf = True,\n",
    "                       sublinear_tf = True, \n",
    "                       stop_words = 'english')\n",
    "\n",
    "#2\n",
    "tfidf.fit(list(X_train) + list(X_test))\n",
    "\n",
    "#3\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Naive Bayes + Tf-idf <a name=\"8\"></a>\n",
    "\n",
    "**Naive Bayes**: classifier based on Bayes' theorem that assumes the presence of a feature (in this case, a word in a label) in a class (the object's place of origin) is unrelated to other features. It is primarily used for text classification with high dimensional training data sets - such as our TF-IDF matrix. \n",
    "\n",
    "There are many variations of the Naive Bayes algorithm (which you can read about [here][1]). In this case, we will use the [multinomial variation][2] where multiple occurrences of a feature have a high impact on the output.\n",
    "\n",
    "For our analysis, we must first initialize a version of the algorithm. **(1)** We can then \"fit\" our training data (X-train_tfidf) and its labels (y_train) into the algorithm - this gives the model our previously-labelled features to study in order to make its category predictions. **(2)**\n",
    "\n",
    "We can then present the full results from the model using **classification_report**. **(3)** Skip to Section 10 for the interpretation of these results. \n",
    "\n",
    "[1]: https://www.hackerearth.com/blog/machine-learning/introduction-naive-bayes-algorithm-codes-python-r/\n",
    "[2]: https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---NB Test Set Results---\n",
      "Accuracy with NB: 0.4859154929577465\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "             amsterdam       0.22      1.00      0.36        20\n",
      "               antwerp       0.00      0.00      0.00         3\n",
      "                arnhem       0.00      0.00      0.00         1\n",
      "              augsburg       0.00      0.00      0.00         2\n",
      "               austria       1.00      1.00      1.00         2\n",
      "               batavia       0.00      0.00      0.00         2\n",
      "                berlin       0.00      0.00      0.00         2\n",
      "            birmingham       1.00      0.20      0.33         5\n",
      "                bruges       1.00      0.75      0.86        12\n",
      "              brussels       1.00      1.00      1.00         1\n",
      "                 china       1.00      0.20      0.33         5\n",
      "                 delft       0.00      0.00      0.00         1\n",
      "            den helder       0.00      0.00      0.00         2\n",
      "              deventer       1.00      0.33      0.50         6\n",
      "             dordrecht       1.00      0.88      0.93         8\n",
      "               england       0.00      0.00      0.00         1\n",
      "                europe       0.00      0.00      0.00         1\n",
      "              flushing       1.00      0.50      0.67         2\n",
      "                france       0.00      0.00      0.00         3\n",
      "               germany       1.00      0.50      0.67         2\n",
      "                 hague       0.00      0.00      0.00         3\n",
      "             harlingen       0.00      0.00      0.00         1\n",
      "               holland       0.00      0.00      0.00         2\n",
      "   hoorn north holland       0.00      0.00      0.00         1\n",
      "indonesian archipelago       0.00      0.00      0.00         1\n",
      "                 italy       0.75      0.53      0.62        17\n",
      "                 japan       1.00      0.14      0.25         7\n",
      "                  java       1.00      0.64      0.78        14\n",
      "                leiden       0.00      0.00      0.00         3\n",
      "             lige city       0.00      0.00      0.00         2\n",
      "                london       0.00      0.00      0.00         1\n",
      "            maastricht       0.00      0.00      0.00         1\n",
      "                madura       1.00      0.83      0.91         6\n",
      "            middelburg       0.00      0.00      0.00         1\n",
      "                naples       0.00      0.00      0.00         1\n",
      "\n",
      "           avg / total       0.61      0.49      0.46       142\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andres/anaconda/envs/Py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 35, does not match size of target_names, 51\n",
      "  .format(len(labels), len(target_names))\n",
      "/Users/andres/anaconda/envs/Py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "NBclf = MultinomialNB()\n",
    "\n",
    "#2\n",
    "NBclf.fit(X_train_tfidf, y_train)\n",
    "y_pred = NBclf.predict(X_test_tfidf)\n",
    "\n",
    "#3\n",
    "print(\"---NB Test Set Results---\")\n",
    "print(\"Accuracy with NB: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Random Forest + Tf-idf <a name=\"9\"></a>\n",
    "**Random Forest**: classifier that creates a series - \"forest\" - of [Decision Trees][1] whose results are then merged to create a more stable, accurate prediction. The algorithm considers a random subset of features with every new tree, ensuring greater feature diversity and overall a better model. Random Forest can work with both classification and regression machine learning problems. \n",
    "\n",
    "See the following [link][2] for a thorough explanation of the model's structure, importance and use cases.\n",
    "\n",
    "The steps to apply the Random Forest classifier are the same as those for Naive Bayes: initializing the model **(1)**, fitting it with the data **(2)** and presenting the results. **(3)**\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Decision_tree#Decision_tree_building_blocks\n",
    "[2]: https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Test Set Results---\n",
      "Accuracy with RF: 0.6126760563380281\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "             amsterdam       0.35      0.85      0.50        20\n",
      "               antwerp       0.67      0.67      0.67         3\n",
      "                arnhem       0.00      0.00      0.00         1\n",
      "              augsburg       0.50      0.50      0.50         2\n",
      "               austria       1.00      1.00      1.00         2\n",
      "               batavia       0.67      1.00      0.80         2\n",
      "                berlin       0.00      0.00      0.00         2\n",
      "            birmingham       0.75      0.60      0.67         5\n",
      "                bruges       1.00      0.75      0.86        12\n",
      "              brussels       0.50      1.00      0.67         1\n",
      "                 china       1.00      0.80      0.89         5\n",
      "                 delft       0.00      0.00      0.00         1\n",
      "            den helder       0.00      0.00      0.00         2\n",
      "              deventer       0.60      0.50      0.55         6\n",
      "             dordrecht       1.00      0.88      0.93         8\n",
      "               england       0.00      0.00      0.00         1\n",
      "                europe       1.00      1.00      1.00         1\n",
      "              flushing       1.00      0.50      0.67         2\n",
      "                france       1.00      1.00      1.00         3\n",
      "               germany       1.00      1.00      1.00         2\n",
      "                 hague       0.67      0.67      0.67         3\n",
      "             harlingen       0.00      0.00      0.00         1\n",
      "               holland       1.00      0.50      0.67         2\n",
      "   hoorn north holland       1.00      1.00      1.00         1\n",
      "indonesian archipelago       0.00      0.00      0.00         1\n",
      "                 italy       0.47      0.41      0.44        17\n",
      "                 japan       0.33      0.14      0.20         7\n",
      "                  java       0.71      0.71      0.71        14\n",
      "                leiden       1.00      0.33      0.50         3\n",
      "             lige city       0.00      0.00      0.00         2\n",
      "                london       1.00      1.00      1.00         1\n",
      "            maastricht       0.00      0.00      0.00         1\n",
      "                madura       1.00      0.83      0.91         6\n",
      "            middelburg       0.00      0.00      0.00         1\n",
      "                naples       0.00      0.00      0.00         1\n",
      "\n",
      "           avg / total       0.63      0.61      0.60       142\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andres/anaconda/envs/Py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 35, does not match size of target_names, 51\n",
      "  .format(len(labels), len(target_names))\n",
      "/Users/andres/anaconda/envs/Py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "RFclf = RandomForestClassifier()\n",
    "\n",
    "#2\n",
    "RFclf.fit(X_train_tfidf, y_train)\n",
    "y_pred = RFclf.predict(X_test_tfidf)\n",
    "\n",
    "#3\n",
    "print(\"---Test Set Results---\")\n",
    "print(\"Accuracy with RF: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparing Results & Improving the Model <a name=\"10\"></a>\n",
    "\n",
    "### Metrics\n",
    "* [Accuracy][1]: the percentage of predictions our model got right. \n",
    "    - Ex. if our model had an accuracy of .65, it correctly classified 65% of all records. \n",
    "* [Precision][2]: the proportion of positive identifications in our model that were correct; calculated as the number of True Positives over Total (True and False) Positives. \n",
    "    - Ex. if the category \"amsterdam\" had a precision of 0.29, it means that, out of all the records our model labelled as \"amsterdam\", only 29% were correct. \n",
    "* [Recall][2]: the proportion of labels that were identified correctly. In other words how many examples of a certain label were accurately predicted vs. how many were missed. \n",
    "    - Ex. if the category \"hague\" had a recall score of 0.33, it means that, out of all the records labelled \"hague\", our model correctly identified 33% of said records, but missed 77% of them. \n",
    "* [F-1 Score][3]: the weighted harmonic mean of our models' precision and recall value - i.e. it conveys the balance between the model's precision and recall metrics. \n",
    "* [Support][4]: number of records for each class (or the entire subset) in the tested dataset. \n",
    "\n",
    "*Note: visit scikit-learn's [documentation][5] for a detailed description of all available statistics.*\n",
    "\n",
    "[1]: https://developers.google.com/machine-learning/crash-course/classification/accuracy\n",
    "[2]: https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n",
    "[3]: https://en.wikipedia.org/wiki/F1_score\n",
    "[4]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support\n",
    "[5]: https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-and-f-measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results \n",
    "\n",
    "The table below puts together the basic prediction statistics from our models: \n",
    "\n",
    "Statistic | NaiveB | RandomF\n",
    ":--- | --- | ---\n",
    "*Accuracy* | 0.486 | 0.613\n",
    "*Avg. Precision* | 0.61 | 0.63\n",
    "*Avg. Recall* | 0.49 | 0.61\n",
    "*Avg. F-1 Score* | 0.46 | 0.60\n",
    "*Support* | 142 | 142\n",
    "\n",
    "Based on these results, our Random Forest produced more accurate predictions on the origin locations of our objects, although not by much. While the metric criteria for a succesful model depends on each project, one should always [consider many different available metrics][1] before making any business decisions based on the algorithm's results. \n",
    "\n",
    "### Improving the Model & Results\n",
    "\n",
    "For best results, always try to...\n",
    "* ...gather as much training data as possible! \n",
    "    - With approximately 2000 records to train and test our model, our excercise was on the lower end of the size spectrum. \n",
    "* ...conduct more text pre-processing, like [stemming or lemmatization][1]\n",
    "    - This can help the model give words like \"sculpture\", \"sculpting\" and \"sculptor\" equal weight in its analysis. \n",
    "* ...experiment with and compare different models according to your machine learning problem. \n",
    "    - For this excercise, we could have also looked into algorithms like [Support Vector Classifier][2], [Logistic Regression][3] or [Xgboost][4]. \n",
    "    - Remember to look into each model's advantages and disadvantages in regards to your data's or inquiry's characteristics.\n",
    "\n",
    "[1]: https://medium.com/greyatom/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b\n",
    "[2]: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "[3]: https://scikit-learn.org/stable/modules/svm.html#classification\n",
    "[4]: https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102\n",
    "[5]: https://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion <a name=\"11\"></a>\n",
    "\n",
    "Implementing machine learning technologies (especially in new sectors) will always be an iterative, exploratory process: AI-based models, implementations and products must depend on extensive brainstorming and testing in order to be succesful.\n",
    "\n",
    "Even though the first iteration of our models was not (numerically) successful, it is important to remember that there is always room for improvement and innovation. Ideally, after this initial set of results, we would continue studying both the data and the algorithms to look for ways in which to refine the model and its subsequent implementation. \n",
    "\n",
    "Do not be afraid to consider questions like...\n",
    "* ...are we looking at the wrong dependent variable and should instead seek to predict object iconography tags?\n",
    "* ...are we asking the wrong question: should we reconfigure our analysis as a regression excercise? \n",
    "* ...do we need to collect more data (either from our own collection or a partner institution)? \n",
    "\n",
    "Regardless of the inquiry, always [make sure the technology follows both the museum's vision and its audience's needs][1] - not the other way around. Machine learning's possibilities are endless, so [guide your AI solutions according to your institution's requirements][2] in order to create valuable experiences for staff and visitors alike. Otherwise, without the human component, your models will only be numbers on a spreadsheet or lines in a function.\n",
    "\n",
    "[1]: https://strategyn.com/outcome-driven-innovation-process/customer-needs/\n",
    "[2]: https://www.huffingtonpost.com/entry/is-your-companys-reaction-to-digital-a-cacophony-or_us_5a31a70ee4b04bd8793e970d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
